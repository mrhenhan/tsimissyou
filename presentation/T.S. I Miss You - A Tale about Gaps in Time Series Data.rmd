---
title: "T.S. I Miss You"
subtitle: "A Tale about Gaps in Time Series Data"
author: "Henrik Hain"
institute: "`Data Scientist@All.In Data GmbH`"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---
name: reachout
# Find me ...
.pull-left[
## today@bitExpert AG
```{r out.width='100%', fig.height=4.5, eval=require('leaflet'), echo=FALSE}
  library(leaflet)
  leaflet() %>% addTiles() %>% addMarkers(lng=8.4672802, lat=49.5124582, popup="bitExpert AG") %>% setView(8.4672802, 49.5124582, zoom = 18)
```
]
.pull-right[
  .right[
    ## always@web
  ]
  .right[
    <img src="img/avatar.jpeg" width="30%" />
  ]
  .right[
  <p>
    <a href="http://twitter.com/mrhenhan">
    <svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg>
    @MrHenHan
    </a>
    <br />
    <a href="http://github.com/mrhenhan">
    <svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
    @MrHenHan
    </a>
   <br />
    <a href="https://henrikhain.io">
    <svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg>
    henrikhain.io
    </a>
  <br />
    <a href="mailto:henrik.hain@all-in-data.de">
    <svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 512 512"><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"></path></svg>
    henrik.hain@all-in-data.de
    </a>
    </p>
  ]
]
---
class: inverse, center, middle

# Univariate Time Series
## Know Your Friend
--

# $\{y_t\}_{t=1}^T$
### and
### $|t_1 - t_2| = |t_2 - t_3| = ... = |t_{n-1} - t_n|$

???
- Series of scalar observations
- At equidistant points in time
---
# Challenges

- ## Special imputation case
- ## No helping covariates
- ## Only time dependencies
- ## Existing solutions mostly address
  - ### Multivariate time series
  - ### Depend on correlations

---
# Visualization
.pull-left[
.center[Airline Passengers - Trend + seasonality]
```{r out.width='100%', dpi=300, fig.height=3, eval=require('TSA'), echo=FALSE}
  data(list = 'airpass')
  par(cex.main = 1.8, cex.lab = 1.4)
  plot(airpass, xlab = "Month", ylab = "No. Air. Pass")
```
.center[S&P Composite Index - Trend]
```{r out.width='100%', dpi=300, fig.height=3, eval=require('TSA'), echo=FALSE}
  data(list = 'SP')
  par(cex.main = 1.8, cex.lab = 1.4)
  plot(SP, xlab = "Quarter", ylab = "Rate S&P")
```
]
.pull-right[
.center[Beer Sales - Seasonality]
```{r out.width='100%', dpi=300, fig.height=3, eval=require('TSA'), echo=FALSE}
  data(list = 'beersales')
  par(cex.main = 1.8, cex.lab = 1.4)
  plot(beersales, xlab = "Month", ylab = "Megabarrel")
```
.center[Google Returns - Nearly white noise]
```{r out.width='100%',dpi=300, fig.height=3, eval=require('TSA'), echo=FALSE}
  data(list = 'google')
  par(cex.main = 1.8, cex.lab = 1.4)
  plot(google, xlab = "Day", ylab = "Returns")
```
]

---
# Analysis 

- ## Identify time series pattern
- ## Identify gap pattern
- ## Selecting imputation method


???
- Irregular parts can be random
  - but can also show autocorrelation or cycles of unpredictable duration
  
---
# Standard Analysis Tools
.pull-left[
.center[
### Decomposition
]
- #### Decompose a time series into level, trend, seasonality, and noise
- #### Additive model (linear)
  - #### $y(t) = t_{level} + t_{trend} + t_{seasonality} + t_{noise}$
- #### Multiplicative model (non-linear)
  - #### $y(t) = t_{level} * t_{trend} * t_{seasonality} * t_{noise}$
- #### Mixtures of both are possible
]
.pull-right[
.center[
### Autocorrelation
]
- #### Correlation time series and lagged version of itself
- #### High correlation values [-1,1]
  - #### Future may be predicted from the past
  - #### Indicator for reliable imputations
]

---
# Air Passenger - Trend, Seasonality
.pull-left[
.center[Loess Decomposition]
```{r eval=require('TSA'),dpi=300, echo=FALSE}
  data(list = 'airpass')
  stlDecomp <- stl(airpass, s.window = 'periodic')
  plot(stlDecomp, main = "Loess Seasonal Decomposition of Air Passenger Dataset")
```
]
.pull-right[
.center[Autocorrelation]
```{r eval=require('TSA'),dpi=300, echo=FALSE}
  data(list = 'airpass')
  acf(airpass)
```
]
---
# S&P Composite Index - Trend, No Seasonality
.pull-left[
.center[Loess Decomposition]
```{r eval=require('TSA'),dpi=300, echo=FALSE}
  data(list = 'SP')
  stlDecomp <- stl(SP, s.window = 'periodic')
  plot(stlDecomp, main = "Loess Seasonal Decomposition of S&P composite index")
```
]
.pull-right[
.center[Autocorrelation]
```{r eval=require('TSA'),dpi=300, echo=FALSE}
  data(list = 'SP')
  acf(SP)
```
]

---
# Beer Sales - No Trend, Seasonality
.pull-left[
.center[Loess Decomposition]
```{r eval=require('TSA'),dpi=300, echo=FALSE}
  data(list = 'beersales')
  stlDecomp <- stl(beersales, s.window = 'periodic')
  plot(stlDecomp, main = "Loess Seasonal Decomposition of beer sales")
```
]
.pull-right[
.center[Autocorrelation]
```{r eval=require('TSA'),dpi=300, echo=FALSE}
  data(list = 'beersales')
  acf(beersales)
```
]

---
# Google Daily Returns - No Trend, No Seasonality
.pull-left[
.center[Loess Decomposition]
```{r eval=require('TSA'),dpi=300, echo=FALSE}
  data(list = 'google')
  stlDecomp <- stl(ts(google, frequency = 13), 'periodic')
  plot(stlDecomp, main = "Loess Seasonal Decomposition of google returns")
```
]
.pull-right[
.center[Autocorrelation]
```{r eval=require('TSA'),dpi=300, echo=FALSE}
  data(list = 'google')
  acf(google)
```
]

---
class: inverse, center, middle
# Missing Values Analysis

---
# Missing Values

.pull-left[
.center[
### Mechanisms
]
- #### Missing completely at random (MCAR)
  - #### $P(r|Y_{obs}, Y_{miss}) = P(r)$
- #### Missing at random (MAR)
  - #### $P(r|Y_{obs}, Y_{miss}) = P(r|Y_{obs})$
- #### Missing not at random (MNAR)
  - #### $P(r|Y_{obs}, Y_{miss}) = P(r|Y_{obs}, Y_{miss})$
]
.pull-right[
.center[
### Simulators
]
- #### e.g Exponential distributed 
  - #### $f(x;\lambda) = \lambda e^{- \lambda x}$
- #### e.g
  - #### some formula
- #### e.g.
  - #### $y_t > c$
]


???
- No systematic mechanism on the way data is missing
- Missing data is independent from values of other variables
- Missing value also independent of the value of the observation itself
- example: 24/7 Sensor data - Due to unknown reasons on random occasions data is missing
- Probability of an observation missing is independent of observation itself (MCAR)
- Is dependent on point of time (as there are no other variables)
- example: Machine sensor data are more likely to be missing on weekends (because of shutdowns)
- Not MCAR and not MAR
- Missing value dependend on the observation itself
- example: Temperature sensor gives no values for temp > 100 degrees

---
# Simulate Missing Data
.pull-left[
- ### Evaluation on real missing data is not possible
- ### Missing data has to be simulated
]
.pull-right[

] 
---
# Performance Evaluation
- ## Measuring imputation performance on real missing data not possible!
- ## Create missing data in $ts_{complete}$ to get $ts_{incomplete}$
- ## Apply imputation algorithm to $ts_{incomplete}$ to get $ts_{imp}$
- ## Compare differences between $ts_{complete}$ and $ts_{imp}$
---
# Evaluation Metrics
- ## Root Mean Square Error (RMSE)
  - ### $RMSE(y,\hat{y}) := \sqrt{{1 \over n} \sum_{t=1}^n (y_t - \hat{y_t})^2}$
- ## Mean Absolute Percentage Error (MAPE)
  - ### $MAPE(y,\hat{y}) := {\frac{1}{n} \sum_{t=1}^n \frac{|y_t - \hat{y}_t|}{|y_t|}}$
- ## Evaluation metric should be application specific!
  
???
- 2 very common evaluation metrics
- RMSE may not be good in data with strong trend
  - Data and imputation error later in trend would have stronger influence than earlier data
  - MAPE is more stable in this case
- in general an evaluation metric should be application/goal  specific

---
class: inverse, center, middle

# Classic Imputation Methods

---

---
class: inverse, center, middle

# Deep Learning Imputation Methods

---
class: inverse, center, middle

# Performance Evaluation - B

---
class: center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), and [R Markdown](https://rmarkdown.rstudio.com).
